{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOV9QMURPR8G2+izZ13i/5k",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/stefarine/DMML2022_ROLEX/blob/main/code/cb.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#DATA LOAD\n"
      ],
      "metadata": {
        "id": "cA0kDe5U0nV0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m spacy download fr_core_news_sm"
      ],
      "metadata": {
        "id": "twmccOT101P8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers plotly==5.8.0 pyyaml==5.4.1 datasets pytorch-lightning > /dev/null 2>&1"
      ],
      "metadata": {
        "id": "rw36A53X1e-4"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "o8ds9UGe0jXp"
      },
      "outputs": [],
      "source": [
        "import gensim\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "import bs4 as bs\n",
        "import urllib.request\n",
        "import spacy\n",
        "import string\n",
        "import math\n",
        "from gensim.models import Word2Vec\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from collections import Counter\n",
        "import fr_core_news_sm"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression, LogisticRegressionCV\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score"
      ],
      "metadata": {
        "id": "PEjLBBNZ08Zx"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pprint import pprint\n",
        "import functools\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "import torch.nn.functional as F\n",
        "import pytorch_lightning as pl\n",
        "from transformers import AutoModelForSequenceClassification, CamembertForMaskedLM, AutoTokenizer, AutoConfig\n",
        "from datasets import load_dataset\n",
        "from sklearn.metrics import confusion_matrix, f1_score\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import plotly.express as px\n",
        "from tqdm.notebook import tqdm"
      ],
      "metadata": {
        "id": "nQKyM2VP1cIU"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('training_data.csv')\n",
        "df_prediction = pd.read_csv('unlabelled_test_data.csv')"
      ],
      "metadata": {
        "id": "3dd6tl-B09CM"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y = df['difficulty']\n",
        "X = df['sentence']\n",
        "X_prediction = df_prediction['sentence']"
      ],
      "metadata": {
        "id": "WzgW7FaI0_mo"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Tokenisation"
      ],
      "metadata": {
        "id": "6_ilqNpI1I6h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## TRAIN SET"
      ],
      "metadata": {
        "id": "OXDIp4hk38nK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "listSentences = []\n",
        "for s in X:\n",
        "  listSentences.append(s)"
      ],
      "metadata": {
        "id": "Oa6OKtIT1x3n"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "listSentences"
      ],
      "metadata": {
        "id": "_kKir_aP2KN0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained('camembert-base')\n",
        "\n",
        "tokenizer_output = tokenizer(\n",
        "    listSentences,\n",
        "    padding=\"max_length\",\n",
        "    truncation=True,\n",
        "    return_tensors=\"pt\"\n",
        ")\n",
        "pprint(tokenizer_output, width=150)"
      ],
      "metadata": {
        "id": "A_NPad7v1NVC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## PREDICTION SET"
      ],
      "metadata": {
        "id": "M2_4ybmU3_99"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "listSentences = []\n",
        "for s in X_prediction:\n",
        "  listSentences.append(s)"
      ],
      "metadata": {
        "id": "gP4qLswU4DU3"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "listSentences"
      ],
      "metadata": {
        "id": "jYPPsNxe4NSm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained('camembert-base')\n",
        "\n",
        "tokenizer_output_prediction = tokenizer(\n",
        "    listSentences,\n",
        "    padding=\"max_length\",\n",
        "    truncation=True,\n",
        "    return_tensors=\"pt\"\n",
        ")\n",
        "pprint(tokenizer_output_prediction, width=150)"
      ],
      "metadata": {
        "id": "4umkNqWo4Qbu"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}